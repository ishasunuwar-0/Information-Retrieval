{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fd5e606",
        "outputId": "fb965fe2-ea22-4e64-95ef-351d2bd2ba0d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63fbff2f",
        "outputId": "ebe3773e-4b25-48d9-8659-6fcda8ef4c1c"
      },
      "source": [
        "import os\n",
        "\n",
        "# List contents of cran.all.1400 folder\n",
        "print(\"Contents of /content/drive/MyDrive/cran.all.1400/\")\n",
        "print(os.listdir('/content/drive/MyDrive/cran.all.1400'))\n",
        "\n",
        "# List contents of cranqrel folder\n",
        "print(\"\\nContents of /content/drive/MyDrive/cranqrel/\")\n",
        "print(os.listdir('/content/drive/MyDrive/cranqrel'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /content/drive/MyDrive/cran.all.1400/\n",
            "['1016.txt', '1002.txt', '1040.txt', '1027.txt', '10.txt', '104.txt', '1006.txt', '1004.txt', '1042.txt', '103.txt', '1034.txt', '1022.txt', '1001.txt', '1.txt', '1032.txt', '1026.txt', '1030.txt', '1010.txt', '100.txt', '1037.txt', '1028.txt', '1019.txt', '1014.txt', '1007.txt', '1041.txt', '1035.txt', '1020.txt', '1045.txt', '1029.txt', '1000.txt', '1036.txt', '1008.txt', '1011.txt', '1044.txt', '1003.txt', '1039.txt', '102.txt', '101.txt', '1038.txt', '1033.txt', '1023.txt', '1009.txt', '1031.txt', '1024.txt', '1046.txt', '1043.txt', '1018.txt', '1012.txt', '1021.txt', '1015.txt', '1017.txt', '1013.txt', '1005.txt', '1025.txt', '1051.txt', '1085.txt', '1060.txt', '1073.txt', '1049.txt', '1062.txt', '106.txt', '1066.txt', '1063.txt', '1047.txt', '1088.txt', '1059.txt', '107.txt', '1064.txt', '1052.txt', '1055.txt', '1061.txt', '1067.txt', '1092.txt', '109.txt', '1071.txt', '1084.txt', '1050.txt', '1070.txt', '1080.txt', '1057.txt', '1077.txt', '1075.txt', '1091.txt', '1056.txt', '1087.txt', '1089.txt', '1086.txt', '1082.txt', '1068.txt', '1072.txt', '1078.txt', '1083.txt', '1090.txt', '1069.txt', '1053.txt', '1065.txt', '1058.txt', '108.txt', '1074.txt', '1054.txt', '1048.txt', '1079.txt', '105.txt', '1076.txt', '1081.txt', '1202.txt', '1206.txt', '1098.txt', '1167.txt', '1172.txt', '1174.txt', '1175.txt', '1134.txt', '116.txt', '1131.txt', '1110.txt', '1144.txt', '1142.txt', '1181.txt', '118.txt', '1097.txt', '1096.txt', '1107.txt', '1168.txt', '1132.txt', '1186.txt', '1122.txt', '1113.txt', '1177.txt', '1093.txt', '1127.txt', '1173.txt', '1124.txt', '1169.txt', '114.txt', '1109.txt', '1188.txt', '1112.txt', '1165.txt', '1192.txt', '1102.txt', '1176.txt', '1139.txt', '1115.txt', '1170.txt', '1180.txt', '1129.txt', '1106.txt', '1190.txt', '1189.txt', '11.txt', '1171.txt', '1138.txt', '1108.txt', '1125.txt', '1178.txt', '1140.txt', '1179.txt', '1114.txt', '1099.txt', '1123.txt', '1116.txt', '1118.txt', '113.txt', '1136.txt', '1184.txt', '1100.txt', '1126.txt', '1111.txt', '112.txt', '1164.txt', '1095.txt', '1119.txt', '1154.txt', '1141.txt', '1183.txt', '1135.txt', '117.txt', '1191.txt', '1187.txt', '1130.txt', '1182.txt', '1166.txt', '1104.txt', '1143.txt', '1147.txt', '1133.txt', '1146.txt', '1101.txt', '1145.txt', '1137.txt', '1185.txt', '1160.txt', '1156.txt', '1152.txt', '1153.txt', '1162.txt', '1159.txt', '1150.txt', '115.txt', '1158.txt', '1155.txt', '1163.txt', '1149.txt', '1157.txt', '1161.txt', '1148.txt', '1151.txt', '1105.txt', '1103.txt', '1117.txt', '1128.txt', '110.txt', '1094.txt', '1120.txt', '111.txt', '1121.txt', '155.txt', '152.txt', '165.txt', '161.txt', '251.txt', '16.txt', '236.txt', '159.txt', '146.txt', '158.txt', '243.txt', '242.txt', '168.txt', '229.txt', '240.txt', '238.txt', '157.txt', '241.txt', '145.txt', '230.txt', '235.txt', '244.txt', '253.txt', '154.txt', '147.txt', '232.txt', '153.txt', '15.txt', '250.txt', '245.txt', '246.txt', '25.txt', '150.txt', '160.txt', '239.txt', '207.txt', '162.txt', '24.txt', '163.txt', '166.txt', '156.txt', '231.txt', '237.txt', '222.txt', '167.txt', '248.txt', '23.txt', '252.txt', '255.txt', '149.txt', '143.txt', '233.txt', '247.txt', '234.txt', '164.txt', '151.txt', '254.txt', '249.txt', '227.txt', '184.txt', '216.txt', '171.txt', '180.txt', '223.txt', '175.txt', '173.txt', '17.txt', '208.txt', '219.txt', '221.txt', '228.txt', '225.txt', '217.txt', '226.txt', '211.txt', '21.txt', '220.txt', '188.txt', '169.txt', '177.txt', '209.txt', '192.txt', '215.txt', '224.txt', '22.txt', '210.txt', '218.txt', '183.txt', '172.txt', '213.txt', '212.txt', '214.txt', '178.txt', '1398.txt', '140.txt', '2.txt', '1391.txt', '148.txt', '189.txt', '1335.txt', '1336.txt', '204.txt', '205.txt', '1399.txt', '1393.txt', '1327.txt', '193.txt', '1397.txt', '200.txt', '1386.txt', '198.txt', '144.txt', '1329.txt', '191.txt', '1388.txt', '194.txt', '202.txt', '1400.txt', '190.txt', '201.txt', '142.txt', '196.txt', '1394.txt', '19.txt', '1328.txt', '199.txt', '206.txt', '20.txt', '18.txt', '203.txt', '197.txt', '195.txt', '141.txt', '1395.txt', '1389.txt', '1331.txt', '174.txt', '170.txt', '1390.txt', '182.txt', '1330.txt', '133.txt', '181.txt', '1396.txt', '1338.txt', '1334.txt', '179.txt', '1322.txt', '1392.txt', '1340.txt', '1323.txt', '1321.txt', '134.txt', '139.txt', '186.txt', '187.txt', '176.txt', '1387.txt', '185.txt', '1324.txt', '14.txt', '1337.txt', '1326.txt', '1325.txt', '1375.txt', '1381.txt', '1318.txt', '138.txt', '1300.txt', '1267.txt', '1301.txt', '126.txt', '1369.txt', '1383.txt', '1261.txt', '1373.txt', '1366.txt', '1368.txt', '1376.txt', '1371.txt', '1385.txt', '1372.txt', '137.txt', '1378.txt', '1380.txt', '1382.txt', '1384.txt', '1370.txt', '1374.txt', '1365.txt', '1379.txt', '1316.txt', '1377.txt', '1367.txt', '1306.txt', '1304.txt', '1264.txt', '1257.txt', '1313.txt', '1266.txt', '13.txt', '1303.txt', '1263.txt', '127.txt', '1268.txt', '1259.txt', '1269.txt', '1275.txt', '1297.txt', '1270.txt', '1362.txt', '136.txt', '1353.txt', '1360.txt', '1358.txt', '1356.txt', '1351.txt', '1344.txt', '1364.txt', '135.txt', '1354.txt', '1359.txt', '1352.txt', '1345.txt', '1346.txt', '1349.txt', '1355.txt', '1347.txt', '1357.txt', '1363.txt', '1339.txt', '1361.txt', '1293.txt', '1343.txt', '1319.txt', '1333.txt', '1310.txt', '1315.txt', '1342.txt', '1320.txt', '1317.txt', '1292.txt', '1348.txt', '1314.txt', '131.txt', '128.txt', '1341.txt', '1302.txt', '132.txt', '1311.txt', '1307.txt', '1295.txt', '1312.txt', '1308.txt', '1332.txt', '1350.txt', '1286.txt', '1309.txt', '1305.txt', '1283.txt', '1294.txt', '1289.txt', '1291.txt', '129.txt', '1298.txt', '1285.txt', '1281.txt', '1288.txt', '1260.txt', '1296.txt', '1290.txt', '1282.txt', '1284.txt', '1280.txt', '130.txt', '1278.txt', '1279.txt', '1287.txt', '1299.txt', '1196.txt', '1273.txt', '1258.txt', '1276.txt', '1277.txt', '1272.txt', '1265.txt', '1271.txt', '1197.txt', '1274.txt', '1262.txt', '1248.txt', '1205.txt', '1219.txt', '1242.txt', '1218.txt', '1210.txt', '120.txt', '1251.txt', '121.txt', '1220.txt', '1213.txt', '1203.txt', '1236.txt', '119.txt', '1198.txt', '1244.txt', '1209.txt', '1199.txt', '1194.txt', '1249.txt', '125.txt', '1195.txt', '1239.txt', '1253.txt', '1238.txt', '1200.txt', '1211.txt', '1237.txt', '1208.txt', '1217.txt', '1225.txt', '12.txt', '1243.txt', '1207.txt', '1204.txt', '1247.txt', '1255.txt', '1246.txt', '1250.txt', '1234.txt', '1224.txt', '1228.txt', '1231.txt', '1221.txt', '1223.txt', '1216.txt', '1227.txt', '123.txt', '1212.txt', '1229.txt', '1230.txt', '1215.txt', '1222.txt', '122.txt', '1232.txt', '1214.txt', '1226.txt', '1240.txt', '1241.txt', '1252.txt', '1235.txt', '1256.txt', '124.txt', '1254.txt', '1233.txt', '1245.txt', '1193.txt', '1201.txt', '573.txt', '561.txt', '517.txt', '569.txt', '554.txt', '523.txt', '562.txt', '566.txt', '558.txt', '572.txt', '53.txt', '52.txt', '555.txt', '58.txt', '533.txt', '570.txt', '578.txt', '576.txt', '57.txt', '515.txt', '519.txt', '530.txt', '520.txt', '524.txt', '518.txt', '531.txt', '563.txt', '536.txt', '564.txt', '567.txt', '553.txt', '537.txt', '575.txt', '560.txt', '552.txt', '559.txt', '56.txt', '557.txt', '579.txt', '556.txt', '535.txt', '550.txt', '577.txt', '574.txt', '568.txt', '565.txt', '534.txt', '571.txt', '580.txt', '502.txt', '539.txt', '55.txt', '514.txt', '546.txt', '501.txt', '512.txt', '540.txt', '541.txt', '494.txt', '503.txt', '547.txt', '548.txt', '545.txt', '538.txt', '542.txt', '549.txt', '54.txt', '543.txt', '508.txt', '504.txt', '544.txt', '551.txt', '449.txt', '434.txt', '325.txt', '482.txt', '492.txt', '49.txt', '480.txt', '44.txt', '446.txt', '447.txt', '487.txt', '484.txt', '493.txt', '442.txt', '341.txt', '335.txt', '488.txt', '481.txt', '329.txt', '483.txt', '479.txt', '476.txt', '322.txt', '443.txt', '491.txt', '511.txt', '337.txt', '48.txt', '485.txt', '34.txt', '473.txt', '328.txt', '444.txt', '507.txt', '472.txt', '490.txt', '33.txt', '330.txt', '336.txt', '516.txt', '527.txt', '339.txt', '430.txt', '532.txt', '489.txt', '477.txt', '340.txt', '525.txt', '332.txt', '526.txt', '323.txt', '528.txt', '51.txt', '513.txt', '478.txt', '338.txt', '509.txt', '529.txt', '448.txt', '474.txt', '475.txt', '521.txt', '510.txt', '522.txt', '486.txt', '343.txt', '441.txt', '334.txt', '464.txt', '499.txt', '460.txt', '463.txt', '498.txt', '47.txt', '495.txt', '451.txt', '5.txt', '462.txt', '458.txt', '452.txt', '471.txt', '417.txt', '465.txt', '419.txt', '45.txt', '455.txt', '456.txt', '457.txt', '459.txt', '50.txt', '454.txt', '461.txt', '412.txt', '469.txt', '466.txt', '450.txt', '41.txt', '500.txt', '453.txt', '428.txt', '497.txt', '506.txt', '467.txt', '468.txt', '421.txt', '422.txt', '470.txt', '46.txt', '496.txt', '426.txt', '505.txt', '435.txt', '431.txt', '440.txt', '432.txt', '438.txt', '436.txt', '433.txt', '445.txt', '437.txt', '439.txt', '43.txt', '429.txt', '381.txt', '414.txt', '425.txt', '420.txt', '406.txt', '378.txt', '423.txt', '410.txt', '366.txt', '413.txt', '411.txt', '418.txt', '409.txt', '384.txt', '416.txt', '42.txt', '424.txt', '407.txt', '383.txt', '379.txt', '376.txt', '367.txt', '415.txt', '427.txt', '390.txt', '396.txt', '404.txt', '408.txt', '39.txt', '401.txt', '391.txt', '389.txt', '402.txt', '394.txt', '399.txt', '397.txt', '393.txt', '395.txt', '392.txt', '4.txt', '405.txt', '398.txt', '40.txt', '387.txt', '388.txt', '403.txt', '400.txt', '386.txt', '385.txt', '382.txt', '368.txt', '375.txt', '380.txt', '37.txt', '370.txt', '353.txt', '347.txt', '361.txt', '349.txt', '360.txt', '365.txt', '35.txt', '356.txt', '374.txt', '38.txt', '348.txt', '342.txt', '363.txt', '359.txt', '346.txt', '358.txt', '354.txt', '344.txt', '351.txt', '324.txt', '331.txt', '350.txt', '357.txt', '345.txt', '333.txt', '377.txt', '36.txt', '326.txt', '352.txt', '327.txt', '373.txt', '372.txt', '362.txt', '371.txt', '364.txt', '369.txt', '355.txt', '304.txt', '31.txt', '302.txt', '314.txt', '300.txt', '308.txt', '320.txt', '309.txt', '312.txt', '311.txt', '313.txt', '305.txt', '319.txt', '315.txt', '301.txt', '307.txt', '317.txt', '303.txt', '321.txt', '316.txt', '3.txt', '32.txt', '318.txt', '310.txt', '306.txt', '293.txt', '290.txt', '297.txt', '298.txt', '288.txt', '28.txt', '286.txt', '281.txt', '279.txt', '285.txt', '289.txt', '283.txt', '291.txt', '295.txt', '292.txt', '29.txt', '287.txt', '294.txt', '284.txt', '280.txt', '30.txt', '282.txt', '299.txt', '296.txt', '263.txt', '267.txt', '270.txt', '26.txt', '273.txt', '277.txt', '274.txt', '260.txt', '265.txt', '261.txt', '276.txt', '268.txt', '256.txt', '262.txt', '269.txt', '264.txt', '259.txt', '271.txt', '266.txt', '278.txt', '27.txt', '258.txt', '257.txt', '272.txt', '275.txt', '722.txt', '708.txt', '713.txt', '704.txt', '72.txt', '719.txt', '718.txt', '714.txt', '706.txt', '720.txt', '723.txt', '709.txt', '725.txt', '716.txt', '715.txt', '707.txt', '724.txt', '729.txt', '727.txt', '710.txt', '712.txt', '728.txt', '71.txt', '726.txt', '685.txt', '698.txt', '717.txt', '711.txt', '721.txt', '73.txt', '693.txt', '639.txt', '702.txt', '69.txt', '700.txt', '687.txt', '690.txt', '647.txt', '629.txt', '686.txt', '694.txt', '688.txt', '684.txt', '699.txt', '627.txt', '628.txt', '640.txt', '638.txt', '691.txt', '696.txt', '637.txt', '692.txt', '636.txt', '635.txt', '703.txt', '7.txt', '689.txt', '632.txt', '630.txt', '697.txt', '695.txt', '70.txt', '701.txt', '631.txt', '705.txt', '668.txt', '670.txt', '674.txt', '622.txt', '667.txt', '678.txt', '623.txt', '608.txt', '615.txt', '67.txt', '663.txt', '680.txt', '602.txt', '675.txt', '676.txt', '610.txt', '679.txt', '609.txt', '669.txt', '617.txt', '606.txt', '665.txt', '681.txt', '601.txt', '621.txt', '671.txt', '605.txt', '673.txt', '666.txt', '620.txt', '677.txt', '68.txt', '603.txt', '683.txt', '672.txt', '661.txt', '614.txt', '612.txt', '604.txt', '611.txt', '616.txt', '618.txt', '662.txt', '607.txt', '61.txt', '641.txt', '624.txt', '634.txt', '643.txt', '626.txt', '633.txt', '63.txt', '64.txt', '642.txt', '644.txt', '625.txt', '645.txt', '646.txt', '682.txt', '655.txt', '660.txt', '654.txt', '658.txt', '581.txt', '659.txt', '584.txt', '649.txt', '66.txt', '6.txt', '593.txt', '650.txt', '59.txt', '592.txt', '652.txt', '585.txt', '599.txt', '657.txt', '588.txt', '597.txt', '583.txt', '582.txt', '595.txt', '594.txt', '598.txt', '591.txt', '596.txt', '648.txt', '65.txt', '587.txt', '589.txt', '60.txt', '590.txt', '656.txt', '651.txt', '600.txt', '586.txt', '653.txt', '664.txt', '613.txt', '619.txt', '62.txt', '882.txt', '870.txt', '861.txt', '814.txt', '797.txt', '830.txt', '86.txt', '805.txt', '801.txt', '820.txt', '863.txt', '803.txt', '872.txt', '87.txt', '865.txt', '804.txt', '829.txt', '878.txt', '866.txt', '869.txt', '83.txt', '862.txt', '833.txt', '824.txt', '808.txt', '802.txt', '860.txt', '858.txt', '806.txt', '874.txt', '873.txt', '811.txt', '857.txt', '807.txt', '831.txt', '88.txt', '795.txt', '812.txt', '834.txt', '81.txt', '82.txt', '871.txt', '817.txt', '8.txt', '815.txt', '867.txt', '799.txt', '832.txt', '875.txt', '828.txt', '868.txt', '879.txt', '800.txt', '810.txt', '836.txt', '819.txt', '816.txt', '855.txt', '825.txt', '826.txt', '818.txt', '813.txt', '835.txt', '827.txt', '859.txt', '854.txt', '809.txt', '821.txt', '864.txt', '850.txt', '796.txt', '877.txt', '798.txt', '823.txt', '822.txt', '876.txt', '851.txt', '769.txt', '756.txt', '754.txt', '846.txt', '853.txt', '755.txt', '843.txt', '752.txt', '771.txt', '764.txt', '847.txt', '765.txt', '751.txt', '838.txt', '761.txt', '841.txt', '844.txt', '84.txt', '85.txt', '842.txt', '757.txt', '849.txt', '762.txt', '848.txt', '77.txt', '839.txt', '840.txt', '837.txt', '845.txt', '767.txt', '856.txt', '852.txt', '763.txt', '760.txt', '766.txt', '76.txt', '792.txt', '772.txt', '768.txt', '78.txt', '793.txt', '790.txt', '753.txt', '759.txt', '787.txt', '785.txt', '80.txt', '791.txt', '794.txt', '79.txt', '778.txt', '782.txt', '773.txt', '777.txt', '783.txt', '784.txt', '786.txt', '779.txt', '776.txt', '781.txt', '780.txt', '788.txt', '774.txt', '758.txt', '775.txt', '770.txt', '789.txt', '75.txt', '749.txt', '747.txt', '744.txt', '750.txt', '748.txt', '733.txt', '743.txt', '735.txt', '739.txt', '745.txt', '742.txt', '74.txt', '746.txt', '741.txt', '732.txt', '740.txt', '738.txt', '730.txt', '736.txt', '734.txt', '731.txt', '737.txt', '975.txt', '976.txt', '978.txt', '977.txt', '982.txt', '974.txt', '980.txt', '969.txt', '988.txt', '984.txt', '985.txt', '98.txt', '97.txt', '972.txt', '971.txt', '970.txt', '973.txt', '979.txt', '981.txt', '986.txt', '989.txt', '968.txt', '983.txt', '987.txt', '958.txt', '990.txt', '99.txt', '962.txt', '998.txt', '994.txt', '946.txt', '954.txt', '960.txt', '952.txt', '999.txt', '96.txt', '963.txt', '959.txt', '947.txt', '997.txt', '996.txt', '949.txt', '953.txt', '957.txt', '95.txt', '993.txt', '961.txt', '955.txt', '991.txt', '948.txt', '966.txt', '964.txt', '967.txt', '992.txt', '956.txt', '965.txt', '950.txt', '995.txt', '931.txt', '944.txt', '942.txt', '935.txt', '941.txt', '938.txt', '945.txt', '925.txt', '936.txt', '94.txt', '934.txt', '937.txt', '951.txt', '930.txt', '932.txt', '927.txt', '940.txt', '93.txt', '926.txt', '928.txt', '907.txt', '902.txt', '92.txt', '918.txt', '894.txt', '883.txt', '886.txt', '910.txt', '906.txt', '885.txt', '911.txt', '921.txt', '903.txt', '916.txt', '904.txt', '89.txt', '905.txt', '892.txt', '917.txt', '915.txt', '914.txt', '922.txt', '881.txt', '920.txt', '897.txt', '898.txt', '912.txt', '909.txt', '913.txt', '899.txt', '919.txt', '908.txt', '887.txt', '891.txt', '91.txt', '924.txt', '943.txt', '939.txt', '923.txt', '933.txt', '929.txt', '9.txt', '893.txt', '880.txt', '884.txt', '890.txt', '901.txt', '900.txt', '889.txt', '90.txt', '896.txt', '895.txt', '888.txt']\n",
            "\n",
            "Contents of /content/drive/MyDrive/cranqrel/\n",
            "['132.txt', '113.txt', '122.txt', '126.txt', '11.txt', '119.txt', '118.txt', '117.txt', '133.txt', '116.txt', '128.txt', '124.txt', '107.txt', '129.txt', '1.txt', '114.txt', '130.txt', '127.txt', '120.txt', '121.txt', '112.txt', '131.txt', '123.txt', '111.txt', '134.txt', '110.txt', '115.txt', '104.txt', '109.txt', '12.txt', '13.txt', '125.txt', '136.txt', '101.txt', '102.txt', '105.txt', '100.txt', '106.txt', '10.txt', '103.txt', '108.txt', '76.txt', '7.txt', '72.txt', '75.txt', '68.txt', '69.txt', '6.txt', '58.txt', '47.txt', '70.txt', '61.txt', '64.txt', '55.txt', '78.txt', '60.txt', '77.txt', '54.txt', '56.txt', '66.txt', '79.txt', '71.txt', '59.txt', '63.txt', '73.txt', '8.txt', '67.txt', '74.txt', '57.txt', '65.txt', '62.txt', '81.txt', '80.txt', '53.txt', '41.txt', '44.txt', '35.txt', '36.txt', '20.txt', '33.txt', '212.txt', '202.txt', '32.txt', '203.txt', '211.txt', '24.txt', '39.txt', '214.txt', '206.txt', '37.txt', '201.txt', '210.txt', '217.txt', '139.txt', '29.txt', '205.txt', '5.txt', '4.txt', '46.txt', '49.txt', '215.txt', '34.txt', '209.txt', '48.txt', '40.txt', '45.txt', '52.txt', '38.txt', '207.txt', '43.txt', '216.txt', '42.txt', '142.txt', '51.txt', '50.txt', '3.txt', '26.txt', '31.txt', '208.txt', '225.txt', '28.txt', '218.txt', '221.txt', '219.txt', '22.txt', '223.txt', '183.txt', '186.txt', '224.txt', '222.txt', '182.txt', '19.txt', '23.txt', '194.txt', '187.txt', '174.txt', '195.txt', '189.txt', '25.txt', '27.txt', '220.txt', '191.txt', '197.txt', '213.txt', '196.txt', '30.txt', '198.txt', '149.txt', '21.txt', '168.txt', '171.txt', '177.txt', '137.txt', '147.txt', '157.txt', '204.txt', '160.txt', '138.txt', '18.txt', '15.txt', '145.txt', '199.txt', '14.txt', '17.txt', '170.txt', '146.txt', '140.txt', '165.txt', '152.txt', '143.txt', '144.txt', '135.txt', '159.txt', '164.txt', '166.txt', '200.txt', '151.txt', '141.txt', '176.txt', '2.txt', '169.txt', '175.txt', '158.txt', '150.txt', '161.txt', '180.txt', '193.txt', '178.txt', '181.txt', '184.txt', '172.txt', '179.txt', '192.txt', '188.txt', '185.txt', '173.txt', '190.txt', '155.txt', '154.txt', '16.txt', '153.txt', '163.txt', '162.txt', '156.txt', '148.txt', '167.txt', '87.txt', '84.txt', '97.txt', '83.txt', '98.txt', '90.txt', '89.txt', '88.txt', '9.txt', '95.txt', '93.txt', '85.txt', '92.txt', '94.txt', '96.txt', '82.txt', '91.txt', '99.txt', '86.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wSHxoEDfS0p",
        "outputId": "c5bcff88-a4d0-4126-c248-0e8a2067451c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1400 documents, 225 queries, 1837 relevance pairs.\n",
            "Vocabulary size: 7008 terms\n",
            "\n",
            "Example query:\n",
            "Q1: what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft .\n",
            "Top 10 results (doc_id, score):\n",
            "    13  0.2168\n",
            "   184  0.2040\n",
            "   486  0.1725\n",
            "    12  0.1485\n",
            "  1268  0.1255\n",
            "    51  0.1127\n",
            "   665  0.1063\n",
            "   878  0.1062\n",
            "   875  0.1059\n",
            "   332  0.0962\n",
            "\n",
            "Evaluation:\n",
            "  Mean Precision@5: 0.3991\n",
            "  Mean Precision@10: 0.2813\n",
            "  MAP (top-100): 0.3674\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Set, Tuple\n",
        "import os\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Document:\n",
        "    doc_id: int\n",
        "    text: str\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Query:\n",
        "    q_id: int\n",
        "    text: str\n",
        "\n",
        "\n",
        "# ---------- Parsing Cranfield files ----------\n",
        "\n",
        "def parse_single_cran_doc_file(file_path: str) -> Document:\n",
        "    \"\"\"\n",
        "    Parse a single .txt file from cran.all.1400, where the filename is the doc_id.\n",
        "    The content of the file is assumed to be the document text.\n",
        "    \"\"\"\n",
        "    doc_id = int(os.path.basename(file_path).split('.')[0])\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        text = f.read().strip()\n",
        "    return Document(doc_id=doc_id, text=text)\n",
        "\n",
        "\n",
        "def parse_cran_all_from_directory(directory_path: str) -> List[Document]:\n",
        "    all_docs: List[Document] = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            doc = parse_single_cran_doc_file(file_path)\n",
        "            all_docs.append(doc)\n",
        "    # Sort documents by doc_id to maintain consistency\n",
        "    all_docs.sort(key=lambda d: d.doc_id)\n",
        "    return all_docs\n",
        "\n",
        "\n",
        "def parse_cran_qry(path: str) -> List[Query]:\n",
        "    \"\"\"\n",
        "    Parse cran.qry into a list of Query(q_id, text).\n",
        "    Expected format: <q_id>\\t<query text>\n",
        "    (Based on observed data in /content/cran.qry.txt)\n",
        "    \"\"\"\n",
        "    queries: List[Query] = []\n",
        "\n",
        "    # print(f\"DEBUG: parse_cran_qry - Attempting to open {path}\")\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            # print(f\"DEBUG: parse_cran_qry - Processing line {line_num}: '{line}'\")\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Assuming format: <id>\\t<query text>\n",
        "            parts = line.split('\\t', 1)\n",
        "            if len(parts) == 2:\n",
        "                try:\n",
        "                    q_id = int(parts[0])\n",
        "                    text = parts[1].strip()\n",
        "                    queries.append(Query(q_id=q_id, text=text))\n",
        "                    # print(f\"DEBUG: parse_cran_qry - Added query q_id: {q_id}, text length: {len(text)}\")\n",
        "                except ValueError:\n",
        "                    print(f\"ERROR: parse_cran_qry - Failed to parse q_id in line {line_num}: '{line}'\")\n",
        "            else:\n",
        "                print(f\"ERROR: parse_cran_qry - Unexpected line format in {path} at line {line_num}: '{line}'\")\n",
        "\n",
        "    # print(f\"DEBUG: parse_cran_qry - Finished processing {path}. Queries parsed: {len(queries)}\")\n",
        "    return queries\n",
        "\n",
        "\n",
        "def parse_cran_qrel(path: str) -> Dict[int, Set[int]]:\n",
        "    \"\"\"\n",
        "    Parse a single cranqrel file (or a file containing qrels for one query)\n",
        "    into dict: q_id -> set(doc_id).\n",
        "    Typical format per line: <q_id> <doc_id> <relevance>\n",
        "    We only need q_id and doc_id.\n",
        "    \"\"\"\n",
        "    qrels: Dict[int, Set[int]] = defaultdict(set)\n",
        "    # print(f\"DEBUG: parse_cran_qrel - Attempting to open {path}\")\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            try:\n",
        "                qid = int(parts[0])\n",
        "                docid = int(parts[1])\n",
        "            except ValueError:\n",
        "                # print(f\"ERROR: parse_cran_qrel - Failed to parse line in {path}: '{line.strip()}'\")\n",
        "                continue\n",
        "            qrels[qid].add(docid)\n",
        "    # print(f\"DEBUG: parse_cran_qrel - Finished processing {path}. Qrels parsed: {len(qrels)}\")\n",
        "    return qrels\n",
        "\n",
        "def parse_cran_qrel_from_directory(directory_path: str) -> Dict[int, Set[int]]:\n",
        "    all_qrels: Dict[int, Set[int]] = defaultdict(set)\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            qrels_from_file = parse_cran_qrel(file_path)\n",
        "            for qid, doc_ids in qrels_from_file.items():\n",
        "                all_qrels[qid].update(doc_ids)\n",
        "    return all_qrels\n",
        "\n",
        "\n",
        "# ---------- Preprocessing ----------\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, stopwords: Set[str] = None):\n",
        "        if stopwords is None:\n",
        "            stopwords = {\n",
        "                \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\",\n",
        "                \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\",\n",
        "                \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\",\n",
        "                \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\",\n",
        "                \"with\", \"we\", \"you\", \"your\", \"from\"\n",
        "            }\n",
        "        self.stopwords = stopwords\n",
        "\n",
        "    def preprocess(self, text: str) -> List[str]:\n",
        "        # lowercase\n",
        "        text = text.lower()\n",
        "        # tokenize: alphabetic words only\n",
        "        tokens = re.findall(r\"[a-z]+\", text)\n",
        "        # remove stopwords\n",
        "        tokens = [t for t in tokens if t not in self.stopwords]\n",
        "        return tokens\n",
        "\n",
        "\n",
        "# ---------- Inverted index + vector space model ----------\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self, preprocessor: Preprocessor):\n",
        "        self.pre = preprocessor\n",
        "        self.doc_ids: List[int] = []\n",
        "        self.vocab: Dict[str, int] = {}\n",
        "        self.id_to_term: List[str] = []\n",
        "        self.postings: Dict[int, List[Tuple[int, int]]] = defaultdict(list)\n",
        "        self.N: int = 0\n",
        "        self.df: Dict[int, int] = {}\n",
        "        self.idf: Dict[int, float] = {}\n",
        "        self.doc_norms: List[float] = []\n",
        "\n",
        "    def build(self, documents: List[Document]) -> None:\n",
        "        self.doc_ids = [d.doc_id for d in documents]\n",
        "        self.N = len(documents)\n",
        "        term_id_counter = 0\n",
        "\n",
        "        # build postings\n",
        "        for doc_index, doc in enumerate(documents):\n",
        "            tokens = self.pre.preprocess(doc.text)\n",
        "            tf_counts = Counter(tokens)\n",
        "            for term, tf in tf_counts.items():\n",
        "                if term not in self.vocab:\n",
        "                    self.vocab[term] = term_id_counter\n",
        "                    self.id_to_term.append(term)\n",
        "                    term_id_counter += 1\n",
        "                tid = self.vocab[term]\n",
        "                self.postings[tid].append((doc_index, tf))\n",
        "\n",
        "        # document frequency and idf\n",
        "        self.df = {tid: len(plist) for tid, plist in self.postings.items()}\n",
        "        self.idf = {\n",
        "            tid: math.log((self.N + 1) / (df + 0.5))\n",
        "            for tid, df in self.df.items()\n",
        "        }\n",
        "\n",
        "        self.doc_norms = [0.0] * self.N\n",
        "        for tid, plist in self.postings.items():\n",
        "            idf = self.idf[tid]\n",
        "            for doc_index, tf in plist:\n",
        "                w_td = (1.0 + math.log(tf)) * idf\n",
        "                self.doc_norms[doc_index] += w_td * w_td\n",
        "        self.doc_norms = [math.sqrt(n) if n > 0 else 0.0 for n in self.doc_norms]\n",
        "\n",
        "    def search(self, query_text: str, k: int = 10) -> List[Tuple[int, float]]:\n",
        "        \"\"\"\n",
        "        Return top-k docs as (doc_id, score) for a query.\n",
        "        \"\"\"\n",
        "        tokens = self.pre.preprocess(query_text)\n",
        "        if not tokens:\n",
        "            return []\n",
        "\n",
        "        tf_q = Counter(tokens)\n",
        "\n",
        "        # compute query weights\n",
        "        q_weights: Dict[int, float] = {}\n",
        "        q_norm = 0.0\n",
        "        for term, tf in tf_q.items():\n",
        "            if term not in self.vocab:\n",
        "                continue\n",
        "            tid = self.vocab[term]\n",
        "            idf = self.idf.get(tid, 0.0)\n",
        "            w_tq = (1.0 + math.log(tf)) * idf\n",
        "            q_weights[tid] = w_tq\n",
        "            q_norm += w_tq * w_tq\n",
        "\n",
        "        q_norm = math.sqrt(q_norm) if q_norm > 0 else 0.0\n",
        "        if q_norm == 0.0:\n",
        "            return []\n",
        "\n",
        "        # accumulate scores\n",
        "        scores: Dict[int, float] = defaultdict(float)\n",
        "        for tid, w_tq in q_weights.items():\n",
        "            idf = self.idf[tid]\n",
        "            for doc_index, tf in self.postings.get(tid, []):\n",
        "                w_td = (1.0 + math.log(tf)) * idf\n",
        "                scores[doc_index] += w_tq * w_td\n",
        "\n",
        "        # convert to cosine similarity and map back to doc_ids\n",
        "        results: List[Tuple[int, float]] = []\n",
        "        for doc_index, numerator in scores.items():\n",
        "            denom = self.doc_norms[doc_index] * q_norm\n",
        "            if denom > 0:\n",
        "                score = numerator / denom\n",
        "                results.append((self.doc_ids[doc_index], score))\n",
        "\n",
        "        results.sort(key=lambda x: x[1], reverse=True)\n",
        "        return results[:k]\n",
        "\n",
        "\n",
        "# ---------- Evaluation metrics ----------\n",
        "\n",
        "def precision_at_k(ranked_doc_ids: List[int], relevant_set: Set[int], k: int) -> float:\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "    hits = 0\n",
        "    for i, doc_id in enumerate(ranked_doc_ids[:k], start=1):\n",
        "        if doc_id in relevant_set:\n",
        "            hits += 1\n",
        "    return hits / k\n",
        "\n",
        "\n",
        "def average_precision(ranked_doc_ids: List[int], relevant_set: Set[int]) -> float:\n",
        "    if not relevant_set:\n",
        "        return 0.0\n",
        "    hits = 0\n",
        "    sum_prec = 0.0\n",
        "    for i, doc_id in enumerate(ranked_doc_ids, start=1):\n",
        "        if doc_id in relevant_set:\n",
        "            hits += 1\n",
        "            sum_prec += hits / i\n",
        "    return sum_prec / len(relevant_set)\n",
        "\n",
        "\n",
        "def mean_average_precision(results_by_qid: Dict[int, List[int]],\n",
        "                           qrels: Dict[int, Set[int]]) -> float:\n",
        "    aps = []\n",
        "    for qid, ranked in results_by_qid.items():\n",
        "        aps.append(average_precision(ranked, qrels.get(qid, set())))\n",
        "    return sum(aps) / len(aps) if aps else 0.0\n",
        "\n",
        "\n",
        "# ---------- Main script ----------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Adjust paths for folders in MyDrive\n",
        "    docs_dir = \"/content/drive/MyDrive/cran.all.1400\"\n",
        "    qrels_dir = \"/content/drive/MyDrive/cranqrel\"\n",
        "    queries_file = \"/content/cran.qry.txt\" # Assuming this is a single file in /content/\n",
        "\n",
        "    # print(f\"DEBUG: Listing contents of {docs_dir}\")\n",
        "    # print(os.listdir(docs_dir))\n",
        "    # print(f\"DEBUG: Listing contents of {qrels_dir}\")\n",
        "    # print(os.listdir(qrels_dir))\n",
        "\n",
        "    docs = parse_cran_all_from_directory(docs_dir)\n",
        "    queries = parse_cran_qry(queries_file)\n",
        "    qrels = parse_cran_qrel_from_directory(qrels_dir)\n",
        "\n",
        "    print(f\"Loaded {len(docs)} documents, {len(queries)} queries, \"\n",
        "          f\"{sum(len(v) for v in qrels.values())} relevance pairs.\")\n",
        "\n",
        "    pre = Preprocessor()\n",
        "    index = InvertedIndex(pre)\n",
        "    index.build(docs)\n",
        "    print(f\"Vocabulary size: {len(index.vocab)} terms\")\n",
        "\n",
        "    # Example query demo\n",
        "    # Only proceed if queries list is not empty\n",
        "    if queries:\n",
        "        example_q = queries[0]\n",
        "        print(\"\\nExample query:\")\n",
        "        print(f\"Q{example_q.q_id}: {example_q.text}\")\n",
        "        example_results = index.search(example_q.text, k=10)\n",
        "        print(\"Top 10 results (doc_id, score):\")\n",
        "        for doc_id, score in example_results:\n",
        "            print(f\"  {doc_id:4d}  {score:.4f}\")\n",
        "    else:\n",
        "        print(\"\\nNo queries loaded, skipping example query demo.\")\n",
        "\n",
        "\n",
        "    # Evaluation on all queries with relevance judgments\n",
        "    results_by_qid: Dict[int, List[int]] = {}\n",
        "    for q in queries:\n",
        "        if q.q_id not in qrels:\n",
        "            # print(f\"DEBUG: Query {q.q_id} has no relevance judgments, skipping.\")\n",
        "            continue\n",
        "        ranked = index.search(q.text, k=100)\n",
        "        ranked_ids = [doc_id for doc_id, _ in ranked]\n",
        "        results_by_qid[q.q_id] = ranked_ids\n",
        "\n",
        "    if results_by_qid:\n",
        "        Ks = [5, 10]\n",
        "        prec_at_k = {k: [] for k in Ks}\n",
        "        for qid, ranked_ids in results_by_qid.items():\n",
        "            rel = qrels.get(qid, set())\n",
        "            for k in Ks:\n",
        "                prec_at_k[k].append(precision_at_k(ranked_ids, rel, k))\n",
        "\n",
        "        map_score = mean_average_precision(results_by_qid, qrels)\n",
        "\n",
        "        print(\"\\nEvaluation:\")\n",
        "        for k in Ks:\n",
        "            avg_p = sum(prec_at_k[k]) / len(prec_at_k[k]) if prec_at_k[k] else 0.0\n",
        "            print(f\"  Mean Precision@{k}: {avg_p:.4f}\")\n",
        "        print(f\"  MAP (top-100): {map_score:.4f}\")\n",
        "    else:\n",
        "        print(\"\\nNo queries with relevance judgments processed for evaluation.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQPnld9Mfpuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}